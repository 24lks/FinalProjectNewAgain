[
  {
    "objectID": "EDA.html",
    "href": "EDA.html",
    "title": "EDA",
    "section": "",
    "text": "Introduction\nThe data we will be looking at is from 70,692 survey responses to the CDC’s Behavioral Risk Factor Surveillance System (BRFSS) 2015, which is a health-related telephone survey that is collected annually by the CDC. This data has an equal 50-50 split of respondents with no diabetes and with either prediabetes or diabetes.\n\n\nResponse/Target Variable\n\nDiabetes_binary: 2 classes. 0 is for no diabetes, and 1 is for prediabetes or diabetes.\n\n\n\nFeature Variables: Here is a selection fo the feature variables that we will focus on for our analysis\n\nHighBP: 0 = no high BP 1 = high BP\nHighChol: 0 = no high cholesterol 1 = high cholesterol\nBMI: Body Mass Index\nPhysActivity: physical activity in past 30 days - not including job 0 = no 1 = yes\nFruits: Consume Fruit 1 or more times per day 0=no 1=yes\nVeggies: Consume veggies 1 or more times per day 0=no 1=yes\nHvyAlcoholConsump: (adult men &gt;=14 drinks per week and adult women&gt;=7 drinks per week) 0 = no 1 = yes\nMentHlth: days of poor mental health scale 1-30 days\nSex: 0 = female 1 = male\n\n\nlibrary(readr)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ purrr     1.1.0\n✔ forcats   1.0.0     ✔ stringr   1.5.2\n✔ ggplot2   4.0.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\n\ndiabetes &lt;- read_csv(\"diabetes_binary_5050split_health_indicators_BRFSS2015.csv\")\n\nRows: 70692 Columns: 22\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (22): Diabetes_binary, HighBP, HighChol, CholCheck, BMI, Smoker, Stroke,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndiabetes&lt;-as.tibble(diabetes)\n\nWarning: `as.tibble()` was deprecated in tibble 2.0.0.\nℹ Please use `as_tibble()` instead.\nℹ The signature and semantics have changed, see `?as_tibble`.\n\nstr(diabetes)\n\ntibble [70,692 × 22] (S3: tbl_df/tbl/data.frame)\n $ Diabetes_binary     : num [1:70692] 0 0 0 0 0 0 0 0 0 0 ...\n $ HighBP              : num [1:70692] 1 1 0 1 0 0 0 0 0 0 ...\n $ HighChol            : num [1:70692] 0 1 0 1 0 0 1 0 0 0 ...\n $ CholCheck           : num [1:70692] 1 1 1 1 1 1 1 1 1 1 ...\n $ BMI                 : num [1:70692] 26 26 26 28 29 18 26 31 32 27 ...\n $ Smoker              : num [1:70692] 0 1 0 1 1 0 1 1 0 1 ...\n $ Stroke              : num [1:70692] 0 1 0 0 0 0 0 0 0 0 ...\n $ HeartDiseaseorAttack: num [1:70692] 0 0 0 0 0 0 0 0 0 0 ...\n $ PhysActivity        : num [1:70692] 1 0 1 1 1 1 1 0 1 0 ...\n $ Fruits              : num [1:70692] 0 1 1 1 1 1 1 1 1 1 ...\n $ Veggies             : num [1:70692] 1 0 1 1 1 1 1 1 1 1 ...\n $ HvyAlcoholConsump   : num [1:70692] 0 0 0 0 0 0 1 0 0 0 ...\n $ AnyHealthcare       : num [1:70692] 1 1 1 1 1 0 1 1 1 1 ...\n $ NoDocbcCost         : num [1:70692] 0 0 0 0 0 0 0 0 0 0 ...\n $ GenHlth             : num [1:70692] 3 3 1 3 2 2 1 4 3 3 ...\n $ MentHlth            : num [1:70692] 5 0 0 0 0 7 0 0 0 0 ...\n $ PhysHlth            : num [1:70692] 30 0 10 3 0 0 0 0 0 6 ...\n $ DiffWalk            : num [1:70692] 0 0 0 0 0 0 0 0 0 0 ...\n $ Sex                 : num [1:70692] 1 1 1 1 0 0 1 1 0 1 ...\n $ Age                 : num [1:70692] 4 12 13 11 8 1 13 6 3 6 ...\n $ Education           : num [1:70692] 6 6 6 6 5 4 5 4 6 4 ...\n $ Income              : num [1:70692] 8 8 8 8 8 7 6 3 8 4 ...\n\n#selecting only the vatriables we want to look at as described in introduction\ndiabetes&lt;-diabetes |&gt;\n  select(Diabetes_binary, HighBP, HighChol, BMI, PhysActivity, Fruits, Veggies, HvyAlcoholConsump, MentHlth, Sex)\nstr(diabetes)\n\ntibble [70,692 × 10] (S3: tbl_df/tbl/data.frame)\n $ Diabetes_binary  : num [1:70692] 0 0 0 0 0 0 0 0 0 0 ...\n $ HighBP           : num [1:70692] 1 1 0 1 0 0 0 0 0 0 ...\n $ HighChol         : num [1:70692] 0 1 0 1 0 0 1 0 0 0 ...\n $ BMI              : num [1:70692] 26 26 26 28 29 18 26 31 32 27 ...\n $ PhysActivity     : num [1:70692] 1 0 1 1 1 1 1 0 1 0 ...\n $ Fruits           : num [1:70692] 0 1 1 1 1 1 1 1 1 1 ...\n $ Veggies          : num [1:70692] 1 0 1 1 1 1 1 1 1 1 ...\n $ HvyAlcoholConsump: num [1:70692] 0 0 0 0 0 0 1 0 0 0 ...\n $ MentHlth         : num [1:70692] 5 0 0 0 0 7 0 0 0 0 ...\n $ Sex              : num [1:70692] 1 1 1 1 0 0 1 1 0 1 ...\n\n#Check for how many NA variables we have\n\ncolSums(is.na(diabetes))\n\n  Diabetes_binary            HighBP          HighChol               BMI \n                0                 0                 0                 0 \n     PhysActivity            Fruits           Veggies HvyAlcoholConsump \n                0                 0                 0                 0 \n         MentHlth               Sex \n                0                 0 \n\n#We have found that there are no missing values for the variables we selected, so that is not something we will have to worry about!\n\n#Currently all of all variables are numeric, but it will make more sense to have several of them as factor variables. Next we will create factor versions of all variables except BMI and Mental Helath\n\ndiabetes &lt;- diabetes |&gt;\n  mutate(\n    DiabetesF = factor(\n      Diabetes_binary,\n      levels = c(0, 1),\n      labels = c(\"No diabetes\", \"Prediabetes/Diabetes\")\n    ),\n    BPF = factor(\n      HighBP,\n      levels = c(0, 1),\n      labels = c(\"No high BP\", \"High BP\")\n    ),\n    CholF = factor(\n      HighChol,\n      levels = c(0, 1),\n      labels = c(\"No high cholesterol\", \"High cholesterol\")\n    ),\n    PhysF = factor(\n      PhysActivity,\n      levels = c(0, 1),\n      labels = c(\"No\", \"Yes\")\n    ),\n    FruitsF = factor(\n      Fruits,\n      levels = c(0, 1),\n      labels = c(\"No\", \"Yes\")\n    ),\n    VeggiesF = factor(\n      Veggies,\n      levels = c(0, 1),\n      labels = c(\"No\", \"Yes\")\n    ),\n    AlcF = factor(\n      HvyAlcoholConsump,\n      levels = c(0, 1),\n      labels = c(\"No\", \"Yes\")\n    ),\n    SexF = factor(\n      Sex,\n      levels = c(0, 1),\n      labels = c(\"Female\", \"Male\")\n    )\n  )\n\ndiabetes&lt;-diabetes |&gt;\n  select(DiabetesF, BPF, CholF, PhysF, FruitsF, VeggiesF, AlcF, SexF, BMI, MentHlth)\n\nstr(diabetes)\n\ntibble [70,692 × 10] (S3: tbl_df/tbl/data.frame)\n $ DiabetesF: Factor w/ 2 levels \"No diabetes\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ BPF      : Factor w/ 2 levels \"No high BP\",\"High BP\": 2 2 1 2 1 1 1 1 1 1 ...\n $ CholF    : Factor w/ 2 levels \"No high cholesterol\",..: 1 2 1 2 1 1 2 1 1 1 ...\n $ PhysF    : Factor w/ 2 levels \"No\",\"Yes\": 2 1 2 2 2 2 2 1 2 1 ...\n $ FruitsF  : Factor w/ 2 levels \"No\",\"Yes\": 1 2 2 2 2 2 2 2 2 2 ...\n $ VeggiesF : Factor w/ 2 levels \"No\",\"Yes\": 2 1 2 2 2 2 2 2 2 2 ...\n $ AlcF     : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 2 1 1 1 ...\n $ SexF     : Factor w/ 2 levels \"Female\",\"Male\": 2 2 2 2 1 1 2 2 1 2 ...\n $ BMI      : num [1:70692] 26 26 26 28 29 18 26 31 32 27 ...\n $ MentHlth : num [1:70692] 5 0 0 0 0 7 0 0 0 0 ...\n\n#Now lets explore some relationships!\n\n\n\nNow lets explore some relationships!\n\n#First, lets look at the BMI of no diabetes vs diabetes/prediabeties respondants\n\ndiabetes |&gt; \n  group_by(DiabetesF) |&gt;\n  summarize(\n    mean_BMI = mean(BMI),\n    sd_BMI = sd(BMI)\n  )\n\n# A tibble: 2 × 3\n  DiabetesF            mean_BMI sd_BMI\n  &lt;fct&gt;                   &lt;dbl&gt;  &lt;dbl&gt;\n1 No diabetes              27.8   6.19\n2 Prediabetes/Diabetes     31.9   7.36\n\n#We see that the mean BMI for those with prediabetes/diabetes is about 4 higher than those with no diabetes.\n\nggplot(diabetes, aes(x = DiabetesF, y = BMI)) +\n  geom_boxplot() +\n  labs(title = \"BMI by Diabetes Status\",\n       x = \"Diabetes Status\",\n       y = \"BMI\") +\n  theme_minimal()\n\n\n\n\n\n\n\ndiabetes |&gt; \n  group_by(DiabetesF) |&gt;\n  summarize(\n    mean_Mental = mean(MentHlth),\n    sd_Mental = sd(MentHlth)\n  )\n\n# A tibble: 2 × 3\n  DiabetesF            mean_Mental sd_Mental\n  &lt;fct&gt;                      &lt;dbl&gt;     &lt;dbl&gt;\n1 No diabetes                 3.04      7.21\n2 Prediabetes/Diabetes        4.46      8.95\n\n#We see that the mean number of days of poor mental health was higer for those with prediabetes/diabetes than those with no diabetes.\n\n\nggplot(diabetes, aes(x = DiabetesF, y = MentHlth)) +\n  geom_boxplot() +\n  labs(title = \"Days of Poor Mental Health vs Diabetes Status\",\n       x = \"Diabetes Status\",\n       y = \"Poor Mental Health Days (past 30)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n#Blood Pressure\n#We see many more prediabetes/diabetes had high blood pressure\n\ndiabetes |&gt;\n  group_by(DiabetesF, BPF) |&gt;\n  summarize(n = n()) |&gt;\n  mutate(percent = n / sum(n))\n\n`summarise()` has grouped output by 'DiabetesF'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 4 × 4\n# Groups:   DiabetesF [2]\n  DiabetesF            BPF            n percent\n  &lt;fct&gt;                &lt;fct&gt;      &lt;int&gt;   &lt;dbl&gt;\n1 No diabetes          No high BP 22118   0.626\n2 No diabetes          High BP    13228   0.374\n3 Prediabetes/Diabetes No high BP  8742   0.247\n4 Prediabetes/Diabetes High BP    26604   0.753\n\n#Fruits\n#Not a huge difference in consuming fruit or not.\ndiabetes |&gt;\n  group_by(DiabetesF, FruitsF) |&gt;\n  summarize(n = n())\n\n`summarise()` has grouped output by 'DiabetesF'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 4 × 3\n# Groups:   DiabetesF [2]\n  DiabetesF            FruitsF     n\n  &lt;fct&gt;                &lt;fct&gt;   &lt;int&gt;\n1 No diabetes          No      12790\n2 No diabetes          Yes     22556\n3 Prediabetes/Diabetes No      14653\n4 Prediabetes/Diabetes Yes     20693\n\n#Veggies\n#Not a huge difference in consuming veggies or not.\ndiabetes |&gt;\n  group_by(DiabetesF, VeggiesF) |&gt;\n  summarize(n = n())\n\n`summarise()` has grouped output by 'DiabetesF'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 4 × 3\n# Groups:   DiabetesF [2]\n  DiabetesF            VeggiesF     n\n  &lt;fct&gt;                &lt;fct&gt;    &lt;int&gt;\n1 No diabetes          No        6322\n2 No diabetes          Yes      29024\n3 Prediabetes/Diabetes No        8610\n4 Prediabetes/Diabetes Yes      26736\n\n#Alcohol\n#More heavy alchol consumption in the no diabetes group\ndiabetes |&gt;\n  group_by(DiabetesF, AlcF) |&gt;\n  summarize(n = n())\n\n`summarise()` has grouped output by 'DiabetesF'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 4 × 3\n# Groups:   DiabetesF [2]\n  DiabetesF            AlcF      n\n  &lt;fct&gt;                &lt;fct&gt; &lt;int&gt;\n1 No diabetes          No    33158\n2 No diabetes          Yes    2188\n3 Prediabetes/Diabetes No    34514\n4 Prediabetes/Diabetes Yes     832\n\n#Sex\n#slightly more males\ndiabetes |&gt;\n  group_by(DiabetesF, SexF) |&gt;\n  summarize(n = n())\n\n`summarise()` has grouped output by 'DiabetesF'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 4 × 3\n# Groups:   DiabetesF [2]\n  DiabetesF            SexF       n\n  &lt;fct&gt;                &lt;fct&gt;  &lt;int&gt;\n1 No diabetes          Female 19975\n2 No diabetes          Male   15371\n3 Prediabetes/Diabetes Female 18411\n4 Prediabetes/Diabetes Male   16935\n\n# Plot each factor\n\np1 &lt;- ggplot(diabetes, aes(x = BPF, fill = DiabetesF)) +\n  geom_bar(position = \"fill\") +\n  scale_y_continuous(labels = scales::percent) +\n  labs(title = \"High Blood Pressure\", x = \"\", y = \"Proportion\") +\n  theme_minimal()\n\np2 &lt;- ggplot(diabetes, aes(x = CholF, fill = DiabetesF)) +\n  geom_bar(position = \"fill\") +\n  scale_y_continuous(labels = scales::percent) +\n  labs(title = \"High Cholesterol\", x = \"\", y = \"Proportion\") +\n  theme_minimal()\n\np3 &lt;- ggplot(diabetes, aes(x = PhysF, fill = DiabetesF)) +\n  geom_bar(position = \"fill\") +\n  scale_y_continuous(labels = scales::percent) +\n  labs(title = \"Physical Activity\", x = \"\", y = \"Proportion\") +\n  theme_minimal()\n\nprint(p1)\n\n\n\n\n\n\n\nprint(p2)\n\n\n\n\n\n\n\nprint(p3)\n\n\n\n\n\n\n\n#More high blood pressure with the diabetes group\n#More high chloesterol pressure with the diabetes group\n#More exercise with no diabetes group\n\n\n\nggplot(diabetes, aes(x = BPF, fill = CholF)) +\n  geom_bar(position = \"dodge\") +\n  facet_wrap(~ DiabetesF) +\n  labs(title = \"High BP vs High Cholesterol by Diabetes Status\",\n       x = \"High Blood Pressure\",\n       y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\nggplot(diabetes, aes(x = PhysF, fill = FruitsF)) +\n  geom_bar(position = \"dodge\") +\n  facet_wrap(~ DiabetesF) +\n  labs(title = \"Physical Activity vs Fruit Intake by Diabetes Status\",\n       x = \"Physical Activity\",\n       y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n#Link to Modeling Page\nClick here for the Modeling Page"
  },
  {
    "objectID": "EDA.html#quarto",
    "href": "EDA.html#quarto",
    "title": "EDA",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "EDA.html#running-code",
    "href": "EDA.html#running-code",
    "title": "EDA",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n2 * 2\n\n[1] 4"
  },
  {
    "objectID": "Modeling.html",
    "href": "Modeling.html",
    "title": "Modeling",
    "section": "",
    "text": "Introduction\nThe data we will be looking at is from 70,692 survey responses to the CDC’s Behavioral Risk Factor Surveillance System (BRFSS) 2015, which is a health-related telephone survey that is collected annually by the CDC. This data has an equal 50-50 split of respondents with no diabetes and with either prediabetes or diabetes.\nOur goal is to create models for predicting the Diabetes variable. We’ll look at a few models and then select the best model.\n\n\nSplit the Data\n• We will use functions from tidymodels to split the data into a training and test set (70/30 split). • On the training set, we will create a 5 fold CV split.\n\nlibrary(ranger)\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.4.1 ──\n\n\n✔ broom        1.0.10     ✔ recipes      1.3.1 \n✔ dials        1.4.2      ✔ rsample      1.3.1 \n✔ dplyr        1.1.4      ✔ tailor       0.1.0 \n✔ ggplot2      4.0.0      ✔ tidyr        1.3.1 \n✔ infer        1.0.9      ✔ tune         2.0.1 \n✔ modeldata    1.5.1      ✔ workflows    1.3.0 \n✔ parsnip      1.3.3      ✔ workflowsets 1.1.1 \n✔ purrr        1.1.0      ✔ yardstick    1.3.2 \n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ stringr   1.5.2\n✔ lubridate 1.9.4     ✔ tibble    3.2.1\n✔ readr     2.1.5     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ readr::col_factor() masks scales::col_factor()\n✖ purrr::discard()    masks scales::discard()\n✖ dplyr::filter()     masks stats::filter()\n✖ stringr::fixed()    masks recipes::fixed()\n✖ dplyr::lag()        masks stats::lag()\n✖ readr::spec()       masks yardstick::spec()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndiabetes &lt;- read_csv(\"diabetes_binary_5050split_health_indicators_BRFSS2015.csv\")\n\nRows: 70692 Columns: 22\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (22): Diabetes_binary, HighBP, HighChol, CholCheck, BMI, Smoker, Stroke,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndiabetes&lt;-as.tibble(diabetes)\n\nWarning: `as.tibble()` was deprecated in tibble 2.0.0.\nℹ Please use `as_tibble()` instead.\nℹ The signature and semantics have changed, see `?as_tibble`.\n\nstr(diabetes)\n\ntibble [70,692 × 22] (S3: tbl_df/tbl/data.frame)\n $ Diabetes_binary     : num [1:70692] 0 0 0 0 0 0 0 0 0 0 ...\n $ HighBP              : num [1:70692] 1 1 0 1 0 0 0 0 0 0 ...\n $ HighChol            : num [1:70692] 0 1 0 1 0 0 1 0 0 0 ...\n $ CholCheck           : num [1:70692] 1 1 1 1 1 1 1 1 1 1 ...\n $ BMI                 : num [1:70692] 26 26 26 28 29 18 26 31 32 27 ...\n $ Smoker              : num [1:70692] 0 1 0 1 1 0 1 1 0 1 ...\n $ Stroke              : num [1:70692] 0 1 0 0 0 0 0 0 0 0 ...\n $ HeartDiseaseorAttack: num [1:70692] 0 0 0 0 0 0 0 0 0 0 ...\n $ PhysActivity        : num [1:70692] 1 0 1 1 1 1 1 0 1 0 ...\n $ Fruits              : num [1:70692] 0 1 1 1 1 1 1 1 1 1 ...\n $ Veggies             : num [1:70692] 1 0 1 1 1 1 1 1 1 1 ...\n $ HvyAlcoholConsump   : num [1:70692] 0 0 0 0 0 0 1 0 0 0 ...\n $ AnyHealthcare       : num [1:70692] 1 1 1 1 1 0 1 1 1 1 ...\n $ NoDocbcCost         : num [1:70692] 0 0 0 0 0 0 0 0 0 0 ...\n $ GenHlth             : num [1:70692] 3 3 1 3 2 2 1 4 3 3 ...\n $ MentHlth            : num [1:70692] 5 0 0 0 0 7 0 0 0 0 ...\n $ PhysHlth            : num [1:70692] 30 0 10 3 0 0 0 0 0 6 ...\n $ DiffWalk            : num [1:70692] 0 0 0 0 0 0 0 0 0 0 ...\n $ Sex                 : num [1:70692] 1 1 1 1 0 0 1 1 0 1 ...\n $ Age                 : num [1:70692] 4 12 13 11 8 1 13 6 3 6 ...\n $ Education           : num [1:70692] 6 6 6 6 5 4 5 4 6 4 ...\n $ Income              : num [1:70692] 8 8 8 8 8 7 6 3 8 4 ...\n\n#selecting only the vatriables we want to look at as described in introduction\ndiabetes&lt;-diabetes |&gt;\n  select(Diabetes_binary, HighBP, HighChol, BMI, PhysActivity, Fruits, Veggies, HvyAlcoholConsump, MentHlth, Sex)\nstr(diabetes)\n\ntibble [70,692 × 10] (S3: tbl_df/tbl/data.frame)\n $ Diabetes_binary  : num [1:70692] 0 0 0 0 0 0 0 0 0 0 ...\n $ HighBP           : num [1:70692] 1 1 0 1 0 0 0 0 0 0 ...\n $ HighChol         : num [1:70692] 0 1 0 1 0 0 1 0 0 0 ...\n $ BMI              : num [1:70692] 26 26 26 28 29 18 26 31 32 27 ...\n $ PhysActivity     : num [1:70692] 1 0 1 1 1 1 1 0 1 0 ...\n $ Fruits           : num [1:70692] 0 1 1 1 1 1 1 1 1 1 ...\n $ Veggies          : num [1:70692] 1 0 1 1 1 1 1 1 1 1 ...\n $ HvyAlcoholConsump: num [1:70692] 0 0 0 0 0 0 1 0 0 0 ...\n $ MentHlth         : num [1:70692] 5 0 0 0 0 7 0 0 0 0 ...\n $ Sex              : num [1:70692] 1 1 1 1 0 0 1 1 0 1 ...\n\n#Check for how many NA variables we have\n\ncolSums(is.na(diabetes))\n\n  Diabetes_binary            HighBP          HighChol               BMI \n                0                 0                 0                 0 \n     PhysActivity            Fruits           Veggies HvyAlcoholConsump \n                0                 0                 0                 0 \n         MentHlth               Sex \n                0                 0 \n\n#We have found that there are no missing values for the variables we selected, so that is not something we will have to worry about!\n\n#Currently all of all variables are numeric, but it will make more sense to have several of them as factor variables. Next we will create factor versions of all variables except BMI and Mental Helath\n\ndiabetes &lt;- diabetes |&gt;\n  mutate(\n    DiabetesF = factor(\n      Diabetes_binary,\n      levels = c(0, 1),\n      labels = c(\"No diabetes\", \"Prediabetes/Diabetes\")\n    ),\n    BPF = factor(\n      HighBP,\n      levels = c(0, 1),\n      labels = c(\"No high BP\", \"High BP\")\n    ),\n    CholF = factor(\n      HighChol,\n      levels = c(0, 1),\n      labels = c(\"No high cholesterol\", \"High cholesterol\")\n    ),\n    PhysF = factor(\n      PhysActivity,\n      levels = c(0, 1),\n      labels = c(\"No\", \"Yes\")\n    ),\n    FruitsF = factor(\n      Fruits,\n      levels = c(0, 1),\n      labels = c(\"No\", \"Yes\")\n    ),\n    VeggiesF = factor(\n      Veggies,\n      levels = c(0, 1),\n      labels = c(\"No\", \"Yes\")\n    ),\n    AlcF = factor(\n      HvyAlcoholConsump,\n      levels = c(0, 1),\n      labels = c(\"No\", \"Yes\")\n    ),\n    SexF = factor(\n      Sex,\n      levels = c(0, 1),\n      labels = c(\"Female\", \"Male\")\n    )\n  )\n\ndiabetes&lt;-diabetes |&gt;\n  select(DiabetesF, BPF, CholF, PhysF, FruitsF, VeggiesF, AlcF, SexF, BMI, MentHlth)\n\nstr(diabetes)\n\ntibble [70,692 × 10] (S3: tbl_df/tbl/data.frame)\n $ DiabetesF: Factor w/ 2 levels \"No diabetes\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ BPF      : Factor w/ 2 levels \"No high BP\",\"High BP\": 2 2 1 2 1 1 1 1 1 1 ...\n $ CholF    : Factor w/ 2 levels \"No high cholesterol\",..: 1 2 1 2 1 1 2 1 1 1 ...\n $ PhysF    : Factor w/ 2 levels \"No\",\"Yes\": 2 1 2 2 2 2 2 1 2 1 ...\n $ FruitsF  : Factor w/ 2 levels \"No\",\"Yes\": 1 2 2 2 2 2 2 2 2 2 ...\n $ VeggiesF : Factor w/ 2 levels \"No\",\"Yes\": 2 1 2 2 2 2 2 2 2 2 ...\n $ AlcF     : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 2 1 1 1 ...\n $ SexF     : Factor w/ 2 levels \"Female\",\"Male\": 2 2 2 2 1 1 2 2 1 2 ...\n $ BMI      : num [1:70692] 26 26 26 28 29 18 26 31 32 27 ...\n $ MentHlth : num [1:70692] 5 0 0 0 0 7 0 0 0 0 ...\n\n\n\nset.seed(11)\ndiabetes_split &lt;- initial_split(diabetes, prop = 0.70)\ndiabetes_train &lt;- training(diabetes_split)\ndiabetes_test &lt;- testing(diabetes_split)\ndiabetes_5_fold &lt;- vfold_cv(diabetes_train, 5)\n\n\n\nClassification Tree\nClassification Trees are a type of tree based method for modeling. Tree based methods split up the predictor space into regions, and on each region a different prediction can be made. Classification trees are used when the goal is to predict grpup membership. This is usually done by using the most prevalent class in the region as the prediction.\n\n# Recipe\nctree_rec &lt;- recipe(DiabetesF ~ BPF + CholF + PhysF + FruitsF + VeggiesF + AlcF + SexF + BMI + MentHlth,\n                    data = diabetes_train)\n\n# Model\nctree_mod &lt;- decision_tree(tree_depth = tune(),\n                           min_n = 20,\n                           cost_complexity = tune()) |&gt;\n  set_engine(\"rpart\") |&gt;\n  set_mode(\"classification\")\n\n\n# Workflow\nctree_wkf &lt;- workflow() |&gt;\n  add_recipe(ctree_rec) |&gt;\n  add_model(ctree_mod)\n\n# Tune using defaults first\ntemp &lt;- ctree_wkf |&gt; \n  tune_grid(resamples = diabetes_5_fold,\n           metrics = metric_set(mn_log_loss))  # classification metrics\ntemp |&gt; \n  collect_metrics()\n\n# A tibble: 10 × 8\n   cost_complexity tree_depth .metric     .estimator  mean     n std_err .config\n             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;  \n 1    0.0000000001          5 mn_log_loss binary     0.582     5 0.00140 pre0_m…\n 2    0.000000001          11 mn_log_loss binary     0.589     5 0.00222 pre0_m…\n 3    0.00000001            1 mn_log_loss binary     0.619     5 0.00120 pre0_m…\n 4    0.0000001             7 mn_log_loss binary     0.575     5 0.00157 pre0_m…\n 5    0.000001             13 mn_log_loss binary     0.599     5 0.00367 pre0_m…\n 6    0.00001               2 mn_log_loss binary     0.619     5 0.00120 pre0_m…\n 7    0.0001                8 mn_log_loss binary     0.579     5 0.00206 pre0_m…\n 8    0.001                15 mn_log_loss binary     0.585     5 0.00129 pre0_m…\n 9    0.01                  4 mn_log_loss binary     0.619     5 0.00120 pre0_m…\n10    0.1                  10 mn_log_loss binary     0.619     5 0.00120 pre0_m…\n\n# Define grid for tuning\ntree_grid &lt;- grid_regular(cost_complexity(),\n                          tree_depth(),\n                          levels = c(cost_complexity = 10, tree_depth = 5))\n\n# Tune over grid\ntree_fits &lt;- ctree_wkf |&gt; \n  tune_grid(resamples = diabetes_5_fold,\n            grid = tree_grid,\n            metrics = metric_set(mn_log_loss))\ntree_fits\n\n# Tuning results\n# 5-fold cross-validation \n# A tibble: 5 × 4\n  splits               id    .metrics          .notes          \n  &lt;list&gt;               &lt;chr&gt; &lt;list&gt;            &lt;list&gt;          \n1 &lt;split [39587/9897]&gt; Fold1 &lt;tibble [50 × 6]&gt; &lt;tibble [0 × 4]&gt;\n2 &lt;split [39587/9897]&gt; Fold2 &lt;tibble [50 × 6]&gt; &lt;tibble [0 × 4]&gt;\n3 &lt;split [39587/9897]&gt; Fold3 &lt;tibble [50 × 6]&gt; &lt;tibble [0 × 4]&gt;\n4 &lt;split [39587/9897]&gt; Fold4 &lt;tibble [50 × 6]&gt; &lt;tibble [0 × 4]&gt;\n5 &lt;split [39588/9896]&gt; Fold5 &lt;tibble [50 × 6]&gt; &lt;tibble [0 × 4]&gt;\n\n# Collect metrics\ntree_fits |&gt; \n  collect_metrics()\n\n# A tibble: 50 × 8\n   cost_complexity tree_depth .metric     .estimator  mean     n std_err .config\n             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;  \n 1    0.0000000001          1 mn_log_loss binary     0.619     5 0.00120 pre0_m…\n 2    0.0000000001          4 mn_log_loss binary     0.588     5 0.00159 pre0_m…\n 3    0.0000000001          8 mn_log_loss binary     0.577     5 0.00179 pre0_m…\n 4    0.0000000001         11 mn_log_loss binary     0.589     5 0.00222 pre0_m…\n 5    0.0000000001         15 mn_log_loss binary     0.604     5 0.00451 pre0_m…\n 6    0.000000001           1 mn_log_loss binary     0.619     5 0.00120 pre0_m…\n 7    0.000000001           4 mn_log_loss binary     0.588     5 0.00159 pre0_m…\n 8    0.000000001           8 mn_log_loss binary     0.577     5 0.00179 pre0_m…\n 9    0.000000001          11 mn_log_loss binary     0.589     5 0.00222 pre0_m…\n10    0.000000001          15 mn_log_loss binary     0.604     5 0.00451 pre0_m…\n# ℹ 40 more rows\n\n# Plot metrics vs parameters\ntree_fits |&gt; \n  collect_metrics() |&gt;\n  mutate(tree_depth = factor(tree_depth)) |&gt;\n  ggplot(aes(cost_complexity, mean, color = tree_depth)) +\n  geom_line(size = 1.5, alpha = 0.6) +\n  geom_point(size = 2) +\n  facet_wrap(~ .metric, scales = \"free\", nrow = 2) +\n  scale_x_log10(labels = scales::label_number()) +\n  scale_color_viridis_d(option = \"plasma\", begin = .9, end = 0)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\ntree_fits |&gt;\n  collect_metrics() |&gt;\n  mutate(tree_depth = factor(tree_depth)) |&gt;\n  ggplot(aes(cost_complexity, mean, color = tree_depth)) +\n  geom_line(size = 1.5, alpha = 0.6) +\n  geom_point(size = 2) +\n  facet_wrap(~ .metric, scales = \"free\", nrow = 2) +\n  scale_x_log10(labels = scales::label_number()) +\n  scale_color_viridis_d(option = \"plasma\", begin = .9, end = 0)\n\n\n\n\n\n\n\n# Arrange results by accuracy\ntree_fits |&gt; \n  collect_metrics() |&gt;\n  filter(.metric == \"accuracy\") |&gt;\n  arrange(desc(mean))\n\n# A tibble: 0 × 8\n# ℹ 8 variables: cost_complexity &lt;dbl&gt;, tree_depth &lt;int&gt;, .metric &lt;chr&gt;,\n#   .estimator &lt;chr&gt;, mean &lt;dbl&gt;, n &lt;int&gt;, std_err &lt;dbl&gt;, .config &lt;chr&gt;\n\n# Select best parameters\ntree_best_params &lt;- select_best(tree_fits, metric = \"mn_log_loss\")\ntree_best_params\n\n# A tibble: 1 × 3\n  cost_complexity tree_depth .config         \n            &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;           \n1    0.0000000001          8 pre0_mod03_post0\n\n#Refit on the entire training set using this tuning parameter\n\nctree_final_wkf &lt;- ctree_wkf |&gt; finalize_workflow(tree_best_params)\n\nctree_final_fit &lt;- ctree_final_wkf |&gt;\n  last_fit(diabetes_split, metrics = metric_set(accuracy, mn_log_loss))\n\ncollect_metrics(ctree_final_fit)\n\n# A tibble: 2 × 4\n  .metric     .estimator .estimate .config        \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;          \n1 accuracy    binary         0.712 pre0_mod0_post0\n2 mn_log_loss binary         0.568 pre0_mod0_post0\n\n\n\n\nRandom Forest Model\nFor a random forest model we will create multiple trees from bootstrap samples and average the results in some way for the final prediction. Random forests do not use all predictors at each step, and considers splits using a random subset of predictors each time, where the number is a tuning parameter. Using a radom forest model makes bagged trees predictions more correlated. By randomly selecting a subset of predictors, a good predictor or two won’t dominate the tree fits!\n\n# Recipe\nRF_rec &lt;- recipe(DiabetesF ~ BPF + CholF + PhysF + FruitsF + VeggiesF + AlcF + SexF + BMI + MentHlth,\n                    data = diabetes_train) |&gt;\n  step_normalize(all_numeric()) |&gt;\n step_dummy(BPF, CholF, PhysF, FruitsF, VeggiesF, AlcF, SexF)\nRF_rec |&gt;\nprep(diabetes_train) |&gt;\n bake(diabetes_train) \n\n# A tibble: 49,484 × 10\n      BMI MentHlth DiabetesF        BPF_High.BP CholF_High.cholesterol PhysF_Yes\n    &lt;dbl&gt;    &lt;dbl&gt; &lt;fct&gt;                  &lt;dbl&gt;                  &lt;dbl&gt;     &lt;dbl&gt;\n 1  0.589   -0.458 Prediabetes/Dia…           1                      1         0\n 2 -1.53    -0.458 Prediabetes/Dia…           0                      1         1\n 3 -0.683   -0.458 No diabetes                1                      1         1\n 4 -0.824   -0.458 No diabetes                1                      0         1\n 5 -0.400   -0.458 Prediabetes/Dia…           0                      1         1\n 6 -0.824    2.01  No diabetes                0                      1         1\n 7 -0.400   -0.458 No diabetes                1                      1         0\n 8  0.447   -0.458 Prediabetes/Dia…           0                      0         1\n 9 -0.541   -0.458 No diabetes                0                      0         1\n10 -0.824    1.51  No diabetes                1                      0         1\n# ℹ 49,474 more rows\n# ℹ 4 more variables: FruitsF_Yes &lt;dbl&gt;, VeggiesF_Yes &lt;dbl&gt;, AlcF_Yes &lt;dbl&gt;,\n#   SexF_Male &lt;dbl&gt;\n\n#Model\n\nrf_mod &lt;- rand_forest(mtry= tune()) |&gt;\n  set_engine(\"ranger\") |&gt;\n  set_mode(\"classification\")\n\n#workflow\n\nrf_wkf &lt;- workflow() |&gt;\n add_recipe(RF_rec) |&gt;\n add_model(rf_mod)\n\n#fit to our CV folds\n\nrf_fit &lt;- rf_wkf |&gt;\n tune_grid(resamples = diabetes_5_fold,\n grid = grid_regular(mtry(range = c(2, 9)), levels = 7),\n metrics = metric_set(accuracy, mn_log_loss))\n\n#Check our metrics across the folds\n\nrf_fit |&gt;\n collect_metrics() |&gt;\n filter(.metric == \"mn_log_loss\") |&gt;\n arrange(mean)\n\n# A tibble: 7 × 7\n   mtry .metric     .estimator  mean     n  std_err .config        \n  &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;          \n1     3 mn_log_loss binary     0.566     5 0.000484 pre0_mod2_post0\n2     2 mn_log_loss binary     0.568     5 0.000482 pre0_mod1_post0\n3     4 mn_log_loss binary     0.573     5 0.000511 pre0_mod3_post0\n4     5 mn_log_loss binary     0.582     5 0.000706 pre0_mod4_post0\n5     6 mn_log_loss binary     0.592     5 0.000780 pre0_mod5_post0\n6     7 mn_log_loss binary     0.600     5 0.000844 pre0_mod6_post0\n7     9 mn_log_loss binary     0.627     5 0.00506  pre0_mod7_post0\n\n# Get our best tuning parameter\n\nrf_best_params &lt;- select_best(rf_fit, metric = \"mn_log_loss\")\nrf_best_params\n\n# A tibble: 1 × 2\n   mtry .config        \n  &lt;int&gt; &lt;chr&gt;          \n1     3 pre0_mod2_post0\n\n#Refit on the entire training set using this tuning parameter\n\nrf_final_wkf &lt;- rf_wkf |&gt;\n finalize_workflow(rf_best_params)\nrf_final_fit &lt;- rf_final_wkf |&gt;\n last_fit(diabetes_split, metrics = metric_set(accuracy, mn_log_loss))\n\ncollect_metrics(rf_final_fit)\n\n# A tibble: 2 × 4\n  .metric     .estimator .estimate .config        \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;          \n1 accuracy    binary         0.713 pre0_mod0_post0\n2 mn_log_loss binary         0.562 pre0_mod0_post0\n\n\n\n\nCompare!!\nNow that we have both done, lets compare and choose our winner! That winning model we will then fit on the entire data set.\n\ncollect_metrics(ctree_final_fit)\n\n# A tibble: 2 × 4\n  .metric     .estimator .estimate .config        \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;          \n1 accuracy    binary         0.712 pre0_mod0_post0\n2 mn_log_loss binary         0.568 pre0_mod0_post0\n\ncollect_metrics(rf_final_fit)\n\n# A tibble: 2 × 4\n  .metric     .estimator .estimate .config        \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;          \n1 accuracy    binary         0.713 pre0_mod0_post0\n2 mn_log_loss binary         0.562 pre0_mod0_post0\n\n\nWe want the smallest log loss as our final winner. The log loss is smaller for the random forest model so that is what we will use! Now we will fit that to the entire data set.\n\n\nFitting to Full Data Set\n\nrf_final_model &lt;- rf_final_wkf |&gt;\n  fit(diabetes)\n\n#save so I can use in API\nsaveRDS(rf_final_model, \"rf_final_model.rds\")"
  },
  {
    "objectID": "Modeling.html#quarto",
    "href": "Modeling.html#quarto",
    "title": "Modeling",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "Modeling.html#running-code",
    "href": "Modeling.html#running-code",
    "title": "Modeling",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  }
]